# HPCC Coordination Meeting: 17 March 2022

The HPC Carpentries Task Force holds coworking hours on the first Thursday,
with two time slots intended to provide adequate coverage for our global
constituency. These two meeting times provide the least-painful coverage for
the six non-polar continents. Folks in Antarctica and aboard a space station
are invited to join whichever is most convenient.

- [12:00 UTC][earlier] &mdash; better for Africa, Asia, and Europe
- [21:00 UTC][evening] &mdash; better for the Americas and Oceania

:::warning Note change of time -- the later meeting is now at 2100 UTC, not the
usual 2200 UTC. ::: A Google calendar has been set up to capture these events,
available [online][gcal] and for [download][ical] for your calendar app.

<!-- Info & Callback links -->

[meet]: https://meet.google.com/gez-aeui-jdx
[phone]: https://tel.meet/gez-aeui-jdx?hs=5
[earlier]:
  https://www.timeanddate.com/worldclock/fixedtime.html?iso=20220317T1200&msg=HPC+Carpentries+Coworking+Hour+1
[evening]:
  https://www.timeanddate.com/worldclock/fixedtime.html?iso=20220317T2100&msg=HPC+Carpentries+Coworking+Hour+2
[last-cowork]: https://codimd.carpentries.org/MnewnhRSQ6iqvEdkZgj2yg
[last-coord]: https://codimd.carpentries.org/eT62gOnGTquEfaqLozRB0g?view
[bof-codi]: https://codimd.carpentries.org/9-Y8OaVIT2qpb_P47TR7Lw?view
[minutes]: https://github.com/hpc-carpentry/coordination/tree/main/minutes
[sc21-milestone]: https://github.com/hpc-carpentry/coordination/milestone/1
[sc21-milestone-long]:
  https://github.com/hpc-carpentry/coordination/milestone/2
[big-pr]: https://github.com/carpentries-incubator/hpc-intro/pull/373

## VTC: **_[Click here to join!][meet]_**

Both meetings will use [Google Meet][meet]. You may also call in by [phone].

#### Previous Meetings for Reference

- [Previous Coordination Meeting][last-coord]
- [Previous Coworking Hour][last-cowork]
- [SC'21 BoF Notes][bof-codi]
  - Milestones (issue lists), both [Near-term][sc21-milestone] and
    [Long][sc21-milestone-long]
- [Archive of old Minutes][minutes]

---

## Agenda

At the [last co-working meeting][last-cowork], which had a definite
Coordination flavor to it, there were action times on going through the HPC
Intro lesson and refactoring it in light of recent decisions made about its
scope and content.

- How's that going?

It seems like a general discussion is in order about what our "product suite"
ought to be. Last time, it seemed we were converging on a model where we would
have a core set of lessons, form HPC-Intro (with a pre-req of Shell Intro) on
to performance measurement and then performance programming. In addition to
this, but not necessarily in workshop sequence, there was interest in
developing lessons using emerging workflows, such as containers and more
data-centric parallel tools like Dask, or possibly more general Map-Reduce-like
things.

- Can we draw some lines around this to organize efforts?

Another month, another conference suggestion from Toby. This time it's
[The UK RSE Conference](https://rsecon2022.society-rse.org/key-dates/).
Proposals open for talks, workshops, panels, posters, etc.

---

## 1200 UTC Session

### Attendees:

- Andrew Reid (he/him), NIST
- Toby Hodges (he/him), The Carpentries
- Trevor Keller (they/them), NIST
- Benson Muite
- Jonathan Guyer (he/him), NIST
- Wirawan Purwanto
- Alan O'Cais

### Scope and outcomes review for HPC Intro

Some progress on this, Trevor says they have a local branch where they are
hacking on this stuff. Collaborative work is not yet underway on this. No
reportable progress at this time. Some assistance might be useful, but the
principal barrier is finding the time to focus on it.

This effort will incorporate some of the feedback from teaching efforts at
NIST.

(NB Feedback from the most recent NIST session has not been captured in the
repo at this point.)

### Broad Structure of our Eventual Offerings

Maybe what we want is a strategic plan?

- Motivating event: what we want from one new lesson discussed at last
  (coworking) meeting was:
  1. HPC Intro: What's a cluster, log in & run some black box to exercise the
     job submission process (using Alan's Amdahl code). Requires evicting some
     material from HPC Intro: MPI stuff, intro to parallel coding, and how to
     figure out what's going on if your code is less performant than you hope.
     - Benson has some good material on looking at assembly code
     - Good GPU material in a PR on HPC Intro (closed) and Novice (open)
  2. HPC Novice: Unclear scope; where does it fit in the grand scheme?
     - [name=Benson]: workshop plan would be Unix Shell, HPC Intro,
       Cloud/Singularity.
       - Rationale for cloud: many people don't have HPC on-site, or have
         loosely coupled jobs; ties in well with data.
     - [name=Andrew]: ~~Cloud~~ Singularity seems more advanced, but it may be
       a blind spot?
     - [name=Jonathan]: Is this "Singularity" is for people to create/setup
       containers or to use/run containers?
       - [name=Benson]: Mainly for people to "run" something already made, not
         to set something new.
     - [name=Wirawan] Cloud is not the same thing as HPC, does cloud in this
       context mean just access to a bunch of workstations, or is there a
       queuing system involved? [With intro-to-HPC, we focus on a system with
       lots of computers (compute nodes) and a job scheduler. Would it be
       detracting from this focus by also adding Cloud into the mix.]
     - [name=Andrew] XSEDE has excellent collection of training resources. AWS
       also have "HPC cluster in a can" that can be instantly stood up and
       accessed via a web interface. Alan also has a similar set up.
     - [name=Trevor] Agree that we need to address people who do not have
       access to HPC; e.g. using cloud to set up a HPC-like environment. We can
       have the infrastructure set up. Buit at the early stage, without
       understanding MPI + prallelization, it will be hard to use HPC.
     - [name=Benson] Some people are ok just running scripts, not to buid
       something with MPI.
     -

* [name=Andrew]: Orthogonal issues:
  1. Local access to resources (run HPC Intro locally?)
  2. Cloud resources, where folks without a cluster can jump to
  3. The resource you're running on is known from the outset. After running HPC
     Intro on that resource, do you want to open the black box to explore
     parallelization; or double down on the black box by introducing
     containers?
* [name=Benson]: concern with opening the black box is learners with weak
  programming background (shell only): they will get frustrated with
  introduction of Python code, and MPI calls. Learning how to use containers
  may be a better use of their time & attention.
* [name=Andrew]: What does the cloud resource look like: Slurm?
  - [name=Benson]: AWS, etc., with Cluster in the Cloud, Magic Castle,
    something similar configured by the coordinator. It should have Slurm.
* [name=Jon]: Amdahl lesson seemed to go less toward the code, more using the
  black box to show scaling & how the queuing system is used. Trajectory is
  understanding performance & scaling, less into the weeds of MPI calls.
  Understanding scaling & performance seems more important than understanding
  containers.
  - Providing the code (black box or otherwise) as containers to run seems
    worthwhile; teaching a lesson about containers seems less aligned with the
    direction.
* [name=Andrew]: refactoring HPC Intro to use Amdahl as a black box is
  underway; goal is to get to scaling & performance.
* [name=Toby]:
  - Workshop coordinators/instructors _will not_ know whether their resource is
    local or cloud-based. In the glorious future, The Carpentries will provide
    a resource so the worldwide community can learn these skills without having
    to buy-in with local resources. Gentle reminder to start thinking about
    that Carpentries-supported future today.
  - Prereq at this point is Shell Novice, which is a nice, compact list.
    Following up with containers is a good next step. Digging into the code
    means adding Python as a prereq.
  - There's precedent for workshops that fork in a few different directions
    (e.g. SWC: shell, python? R?, git? sql?). So, developing a 2- or 4-track
    "workshop" is reasonable. Caution, though, that half a day is not very
    long, and leaving learners demoralized at the end of a workshop may drive
    learners away.
* [name=Andrew]: Agreed; we can configure the lesson materials to use The
  Carpentries' cloud resource by default, with capability to switch to local
  resource if it's available. Instructors will know what resource they have to
  teach to.
* [name=Jon]: "standard" 2-day workshops don't work for NIST; instead of
  packing a lesson into a morning, set up a couple hours every Friday for a
  month to dig through the lesson in detail.
* [name=Andrew]: Proposed Curriculum:

  1. Shell Novice
  2. HPC Intro (Amdahl, Slurm)
  3. Python Novice
  4. HPC Programming (pi.py?)

  or

  1. Shell Novice
  2. HPC Intro
  3. ...
  4. HPC Data

* [name=Toby]: doubtful you can teach Shell _and_ HPC Intro in one day.
* [name=Benson] curriculum: how to _run_ on a cluster, not how to _program_, as
  the default:

  1. Shell
  2. HPC Intro
  3. HPC Containers

  or, a grab-bag: HPC Intro, then resources on MPI from elsewhere.

  There are a lot of canned codes out there that people want to use. Focus on
  using the cluster, instead of programming for the cluster.

* [name=Jon]: Agreed, programming is part of the quiver, but not the core
  lesson.
* [name=Alan]: I wouldn't be so keen to plug containers as a core part, it's
  only one way of running stuff
  - Separate lessons can be written (have been written, some himself: LAMMPS)
    on specific applications, which learners are expected to already be
    familiar with, to guide them deeper into profiling & optimization.

---

## 2100 UTC Session

### Attendees:

- Trevor Keller (they/them), NIST
- Andrew Reid (he/him), NIST

### Scope and outcomes review for HPC Intro

Likely action item is to distribute the review work and try to make it happen
in parallel.

### Broad Structure of our Eventual Offerings

Minor follow-up, the earlier session seemed to converge on using containers as
a "second-tier black box" for more sophisticated things to do on the cluster,
so re-examining the incubator singularity container lesson for content we could
borrow and/or help develop would be a useful step.

Follow-follow-up: we don't want to teach container creation, we just want to
use some software to something practical that we can use to further explore (a)
job submission, (b) scaling, and (c) new concept TBD.

### Elections

Our Steering Committee elections are coming up in May.
