# HPCC Coordination Meeting: 21 April 2022

The HPC Carpentries Task Force holds a coordination meeting on the third
Thursday of each month, with two time slots intended to provide adequate
coverage for our global constituency. These two meeting times provide the
least-painful coverage for the six non-polar continents. Folks in Antarctica
and aboard a space station are invited to join whichever is most convenient.

- [11:00 UTC][earlier] &mdash; better for Africa, Asia, and Europe
- [21:00 UTC][evening] &mdash; better for the Americas and Oceania

:::warning Note change of time -- the later meeting is now at 2100 UTC, not the
usual 2200 UTC. ::: A Google calendar has been set up to capture these events,
available [online][gcal] and for [download][ical] for your calendar app.

<!-- Info & Callback links -->

[meet]: https://meet.google.com/gez-aeui-jdx
[phone]: https://tel.meet/gez-aeui-jdx?hs=5
[earlier]:
  https://www.timeanddate.com/worldclock/fixedtime.html?iso=20220421T1100&msg=HPC+Carpentries+Coordination+Hour+1
[evening]:
  https://www.timeanddate.com/worldclock/fixedtime.html?iso=20220421T2100&msg=HPC+Carpentries+Coordination+Hour+2
[last-cowork]: https://codimd.carpentries.org/d_N1tffYTeOXhenNWrU7hQ?view
[last-coord]: https://codimd.carpentries.org/qiS3Gc_sSh2Hhnn3Ty2CbQ?view
[gcal]:
  https://calendar.google.com/calendar/?cid=bWp0ZWh0ZmEycmVjZGZtNmZjdGUwMWVhdGNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ
[ical]:
  https://calendar.google.com/calendar/ical/mjtehtfa2recdfm6fcte01eatc%40group.calendar.google.com/public/basic.ics
[minutes]: https://github.com/hpc-carpentry/coordination/tree/main/minutes
[website]: https://github.com/hpc-carpentry/hpc-carpentry.github.io
[bof-codi]: https://codimd.carpentries.org/9-Y8OaVIT2qpb_P47TR7Lw?view
[minutes]: https://github.com/hpc-carpentry/coordination/tree/main/minutes
[sc21-milestone]: https://github.com/hpc-carpentry/coordination/milestone/1
[sc21-milestone-long]:
  https://github.com/hpc-carpentry/coordination/milestone/2
[big-pr]: https://github.com/carpentries-incubator/hpc-intro/pull/373
[intro-lesson]: https://github.com/carpentries-incubator/hpc-intro
[refactor-pr]: https://github.com/carpentries-incubator/hpc-intro/pull/389

## VTC: **_[Click here to join!][meet]_**

Both meetings will use [Google Meet][meet]. You may also call in by [phone].

#### Previous Meetings for Reference

- [Previous Coordination Meeting][last-coord]
- [Previous Coworking Hour][last-cowork]
- [SC'21 BoF Notes][bof-codi]
  - Milestones (issue lists), both [Near-term][sc21-milestone] and
    [Long][sc21-milestone-long]
- [Archive of old Minutes][minutes]

---

## Agenda

- By now, hopefully the refactoring [PR][refactor-pr] has been merged, and the
  [HPC Intro][intro-lesson] lesson is much improved. Brief discussion of the
  status of the flagship lesson is appropriate here.
- At the previous [coordination meeting][last-coord], we started to try to
  think about what our "product suite" ought to be. Discussion focused on a
  couple of candidate "tracks" that might make sense. We should pick a track to
  prioritize and begin developing the subsequent lessons more seriously.
  - Software user track: Shell novice, HPC Intro, then maybe HPC Data or a
    container lesson, with the emphasis being on running pre-existing
    executables well on the HPC resource
  - Developer track: Shell novice, HPC Intro, Python Intro, then HPC Python or
    another development lesson.
  - Basket of resources: HPC performance analysis, GPU programming, Chapel?
- All of this will benefit from a larger workforce. We should probably
  publicize our efforts better.
  - [CarpentryCon](https://2022.carpentrycon.org/) Conference is early
    August 2022. Anyone have dates and/or deadlines?
  - The [UK RSE Conference](https://rsecon2022.society-rse.org/key-dates/).
    Call for submissions closes May 2, 2022.
  - [PEARC](https://pearc.acm.org/). 2022 deadlines have passed, but it's
    annual, so 2023?
  - [SC22](https://sc22.supercomputing.org/all-dates-deadlines/). BoF deadline
    is July 1, 2022.
  - Near-future [SIGHPC Education](https://sighpceducation.acm.org/events.html)
    events?
    - Best Practices in HPC Training and Education (Tandem with SC?)
    - HPC Education and Training for Emerging Technologies (Tandem with ISC?)
    - Strategies for Enhancing HPC Education and Training (Tandem with PEARC?)
- Should we consider nontraditional or non-academic outreach channels? Guest
  appearances on HPC podcats? Write-ups by HPCWire? Find out the secret
  identity of HPC Guru on twitter?
- Another way to get a larger workforce or to get more work done is to think
  about fundraising. What problems do we have that could be solved with money,
  and where might money come from?
- We have steering-committee elections coming up in May.

---

## 1100 UTC Session

### Attendees:

- Andrew Reid (he/him), NIST
- Benson Muite, Kichakato Kizito
- Trevor Keller (they/them), NIST
- Alan O'Cais

### The Big HPC-Intro PR

Asking folks to have a look and review this, with a view to merging this. It's
fairly large, and so far only has input from a couple of people, more eyes on
it would be good.

### HPC Carpentries Product Suite

Andrew described the notion in the agenda, having a couple of tracks that we
could package, and also a basket of less well-packaged lessons.

This interacts with funding -- Alan has been in touch with a group that is
assembling a proposal for a European funding opportunity. The nature of this
proposal is that must include HPC (called out in the RFP), but the general
sense is that this is unlikely to contribute to one of the tracks, it's more
likely that it will be a one-off opportunity to contribute to the "basket of
content" lessons. Funding would free up some time for Alan and others in this
space.

Focusing Q: Which lesson is next to move in to the incubator?

If we accept the "track" scheme, and focus on the "user" track, then the
container lesson seems like a logical candidate. Resources we might draw from:

- [PawseySC](https://pawseysc.github.io/singularity-containers/) container
  lesson.
- [Incubator](https://carpentries-incubator.github.io/singularity-introduction/)
  container lesson.

HPC Parallel Novice gets some traction in our workshops, is that a good
candidate?

- [Snakemake](https://github.com/hpc-carpentry/hpc-python), to teach about
  workflows?
  - We already have some Snakemake material in our HPC Python lesson.
- [Nextflow](https://www.nextflow.io/) is also a workflow tool that's popular?
  - We don't have any Nextflow material, but there is
    [material](https://carpentries-incubator.github.io/workflows-nextflow/) in
    the Carpentries incubator.

### Funding Opportunities

Discussed briefly above, how we can move forward depends on people's time,
which might be paid for by external funding, e.g. the thing that Alan is
already doing.

Could we stand up a cloud HPC resource with some money? Get enough money to
make something like this permanent?

Alan has access to several OpenStack systems that could host a lesson,
including a GPU cluster. IB is an option on Azure, but it tends to increase
costs by making inefficient use of nodes -- it's do-able. None of our lessons
really require IB.

An aspect of this is that, if/when we join the Carpentries, we will hand off
the resources we build to Carpentries staffers who will set up resources for
users. Reducing friction in this step would strengthen our case, possibly this
is something we could fund?

- [name=Alan]: funding
  [MagicCastle](https://github.com/ComputeCanada/magic_castle) development to
  improve MCHub (user authentication/management/quotas/accounts) would make
  handing off to The Carpentries easier.
  - [name=FÃ©lix-Antoine Fortin], [@cmd-ntrf](https://github.com/cmd-ntrf)

Funding sources: International funding can be complicated. Google
Summer-of-code? Chan Zuckerberg? R-Studio? NumFOCUS?

All of these things would have a maintenance piece.

### Non-traditional outreach

Possible to try to engage with the user-groups of particular software packages
that are used in HPC, like LAMMPs or VASP, or others? Computational chemistry
(Gromacs, maybe), and bioinformatics and genomics.

- [name=Alan] has [a LAMMPS lesson](https://fzj-jsc.github.io/tuning_lammps/)
  already, taught once

Much ado about Kokkos, which LAMMPS has switched over to from CUDA. New/better
paradigm: abstracts away hardware, memory, etc. from the programmer.

- [Pika](https://github.com/pika-org/pika): fork of HPX focusing on the
  single-node use case complemented by minimal MPI support.
- [HPX](https://hpx.stellar-group.org): a C++ Standard Library for concurrency
  and parallelism.
- Kokkos has a bunch of
  [tutorials](https://github.com/kokkos/kokkos-tutorials). Perhaps direct
  interested learners there?
- [Pawsey Container Lesson](https://pawseysc.github.io/singularity-containers/) -
  would be nice to get an overview of this to possiby include it in the
  "Running on HPC track"
- Nextflow and/or Snakemake?
  - https://carpentries-incubator.github.io/workflows-nextflow/
  - https://carpentries-incubator.github.io/snakemake-novice-bioinformatics/

### Conferences

- [CarpentryCon 2022](https://2022.carpentrycon.org/blog/welcome-carpentrycon-2022/):
  Annjiat is on the Task Force. What do we have to do to get a BoF?
- ISC 2022 -- HPC Education was not accepted this year.
- SC22 (Andrew will not coordinate a BoF, but somebody should).
- Also SC22: Maybe a lower barrier to work with the Best Practices in HPC
  Training and Education folks?

## Action Items

1. [name=Alan]: Review the [Big PR](refactor-pr)
   - Everybody is invited!
2. [name=Trevor]: Diff our
   [HPC Python](https://github.com/hpc-carpentry/hpc-python) and Incubator
   [Bioinformatics Snakemake](https://github.com/carpentries-incubator/snakemake-novice-bioinformatics),
   Incubator
   [Workflow Snakemake](https://github.com/carpentries-incubator/workflows-snakemake),
   possibly
   [Nextflow](https://github.com/carpentries-incubator/workflows-nextflow)
   lessons
3. [name=Alan] will demo MagicCastle during the early session of our Coworking
   Hours on May 5!

---

## 2100 UTC Session

### Attendees:

- Trevor Keller (they/them), NIST
- Andrew Reid (he/him), NIST
- Wirawan Purwanto

#### Focus on Next Lesson:

What about "HPC Python" / Workflow.

Snakemake experience:

- [name=Trevor] Using it as CLI tool

The other alternative lessons:

- [Workflows with Snakemake](https://github.com/carpentries-incubator/workflows-snakemake)
  was forked off HPC Python. Last update was 2 years ago.
- [Snakemake Novice - Bioinformatics](https://github.com/carpentries-incubator/snakemake-novice-bioinformatics)
  is new material, and actively developed.

- Note that the Incubator has lessons for several workflow tools:
  https://github.com/carpentries-incubator?q=workflow

  Other workflow tools: there are 5 oither lessons in the making, for example:

  - nextflow
  - ~~julia-data~~ (still an empty boilerplate)

  so we need to explore to see which one to use

Idea: When introducing workflow (e.g. snakemake), we put "intro to Python" as a
prerequisite, since Snakemake is heavily Python-based (though not exclusively
targeted for driving Python-software only).

Community consideration:

- Apparently, a large number of bioinformatics people in the community, so the
  snakemake/bioinformatics lesson is more active.

- How many of current HPC users are using workflow tools (Snakemake or
  otherwise)? How widespread is the use of workflow as part of scientific work
  best practices.

  - apparently the use of workflow tools as "documentable computing/processing"
    is still not as prevalent as it should be

- Which workflow tool can be used to reach the widest possible audience (i.e.
  not drawing "bioinformatics" community but pushing away "materials"
  community, or vice versa)

#### Best workflow tool characteristics?

- Not too steep on a particular community's specifics
- Imparting general concepts of workflow processing (e.g. "task/data
  dependency" & "task parallel" concepts)

We could use some "blackbox" workload in our lesson, akin to the "Amdahl's law"
code.

#### Using HPC Carpentry to teach HPC best practices?

1. Proper understanding & use of HPC systems

2. Well-documented processing of data & compute (e.g. using workflow tools)

### Other Business

#### Fuzzball: HPC-Cloud convergence?

- [name=Wirawan] shared
  [Greg Kurtzer, **_Fuzzball: HPC 2.0_**](https://www.youtube.com/watch?v=Pbmxq3dg35E)

  > Weâve been using the same base architecture for building HPC systems for
  > almost 30 years and while the capabilities of our systems have increased
  > considerably, we still use the same flat and monolithic architecture of the
  > 1990âs to build our systems. What would the next generation architecture
  > look like? How do we leverage containers to do computing of complex
  > workflows while orchestrating not only jobs, but data? How do we bridge HPC
  > into the 2020âs and make optimal use of multi-clusters and federate these
  > systems into a larger resource to unite on-prem, multi-prem, cloud, and
  > multi-cloud? How do we integrate with these resources in a cloud-native
  > compatible manner supporting CI/CD, DevOps, DevSecOps, compute portals,
  > GUIs, and even mobile? This isnât a bunch of shoelace and duct-tape on top
  > of legacy HPC, this is an entirely new way to think about HPC
  > infrastructure. This is a glimpse into HPC-2.0, coming later in Q1 of 2022.

  This could be a light shed on where HPC and cloud converge. The core concept
  of submitting job (data+computation) and "get the job done"

- [name=Andrew] We have been doing something in the similar spirit with
  web-based image processing, where the processing is a workflow invoking one
  or more containers.

  Similar concept: Dask => parallel computing by breaking big computation into
  task graph; compute is moved to where the data is.

#### Recent Foray into Container

Was compelled to use container recently since a computational package (QMCPack)
has a massive & hard-to-satisfy dependency web.

Container is great for solving a lot of software dependency issues.

But certain issues still persist with containers when it talks to the host
system (OS):

- MPI: Host vs in-container MPI version issue
- CUDA: Host CUDA driver vs in-container CUDA libraries mismatch
