# October 21, 2021 HPC Carpentry Coordination Meeting

## Time and Venue

This "meeting" comprises two parts, to improve coverage for our global
community: 1100 UTC and 2200 UTC. Volunteers willing to host additional
sessions should reach out on the
[Slack](https://swcarpentry.slack.com#hpc-carpentry) or the
[mailing list ](https://carpentries.topicbox.com/groups/discuss-hpc) to notify
potential participants.

- Click to convert
  [11:00 UTC](https://www.timeanddate.com/worldclock/fixedtime.html?msg=HPC+Carpentry+Coordination%2C+1100+UTC&iso=20210415T11&p1=1440)
  to your local time
- Click to convert
  [22:00 UTC](https://www.timeanddate.com/worldclock/fixedtime.html?msg=HPC+Carpentry+Coordination+2200+UTC&iso=20210520T22&p1=%3A)
  to your local time

:::success
[**_Click Here to Join the Meeting!_**](https://meet.google.com/gez-aeui-jdx)

The venue is [Google Meet](https://meet.google.com/gez-aeui-jdx).

:::spoiler You can also call in by phone: Call +1 234-405-0205 PIN: 662 051
554# To view more phone numbers, click this link:
<https://tel.meet/gez-aeui-jdx?hs=5> :::

:::info **Shared Calendar for HPC Carpentry**

If you have not already, please add the
[Google Calendar](https://calendar.google.com/calendar/?cid=bWp0ZWh0ZmEycmVjZGZtNmZjdGUwMWVhdGNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ)
to your scheduling app so that these are not a surprise. You may also import
the
[ical](https://github.com/hpc-carpentry/coordination/files/6497063/revised_20210517.ics.zip)
file directly into your calendar, though this will not receive updates in case
of scheduling upsets. :::

## Agenda

See the
[previous coordination CodiMD](https://codimd.carpentries.org/Mq8jVaepRz2eJL7We2_gKA).
For general process info, see also the
[accepted process proposal](https://codimd.carpentries.org/3VbpZ4C5T5eLD9V_0ZJGng).

### State of the Lessons

- Andrew and Trevor are planning to teach HPC Intro to a group of NIST
  volunteers Oct. 22, and by the time of the meeting, may have some edits or
  feedback on the setup process.
- Last month we looked at Benson Muite's feedback, but it's not clear if any of
  that was captured, or actionable for lesson edits, so I'm mentioning this
  again. I know Benson and Alan did some good follow-up on cluster deployment.
- For the HPC-Shell issue, the meeting announcement asking people to vote did
  not match the structure of the
  [issue](https://github.com/hpc-carpentry/coordination/issues/70) especially
  well, it's not obvious what to vote on. The existing issue has a "temperature
  check" which unanimously favors the second option, so a reasonable strategy
  is to forego the formal process and take this as the intent of the community.
  Next steps in this case is to archive the HPC-Shell lesson and incorporate
  the relevant material into HPC-Intro.
  - New
    [Proposal: Archive hpc-shell and roll content into hpc-intro](https://github.com/hpc-carpentry/coordination/issues/82)

### SC21

- The SC21 BoF is on. Various e-mails from SC21 organizers promise that it will
  be a hybrid thing, so the current plan is to have an initial 20 minute
  presentation by Andrew, in-person in St. Louis, on where/who we are, and then
  Q&A following the
  [planned agenda](https://github.com/hpc-carpentry/coordination/blob/main/conferences/SC21/bof_draft.md)

### CarpentryCon

- There's still an
  [issue](https://github.com/hpc-carpentry/coordination/issues/74) on the repo
  where opinions may be shared on how to prepare for CarpentryCon 2021.

### Site Logistics

- Firstly and probably most importantly, there is a formal
  [proposal](https://github.com/hpc-carpentry/coordination/issues/70) to
  resolve the issue about what to do with HPC-Shell. Community members should
  vote on this before the end of the second meeting.
- There remain a number of proposals on the
  [coordination repo](https://github.com/hpc-carpentry/coordination/issues?q=is%3Aopen+is%3Aissue+label%3Aproposal)
  which fall into the "state of the sites" category. - There is a
  [proposal](https://github.com/hpc-carpentry/coordination/issues/71) set up
  long-lived hosting for workshop materials on behalf of organizers.
  https://swcarpentry.slack.com/archives/CEXAZR52T/p1631057210006400 - We have
  had some discussion about registering workshops with the website. Did the
  [recent lesson](https://swcarpentry.slack.com/archives/CEXAZR52T/p1631057210006400)
  do this? What were the barriers? Also, the question was actually about a
  standard feedback form -- should we have this?

### Other Business

---

### 1100 UTC session

#### Attending

- Andrew Reid (he/him), NIST
- Trevor Keller (they/them), NIST
- Alan O'Cais (he/him), JSC
- Benson Muite

#### HPC Chemistry Workshop

#### NIST Workshop

Wrote a Challenge on the
[bash prompt](https://hpc-workshops.github.io/ctcms-hpc-intro/12-cluster/index.html),
including a table of common macros & color codes, since our default
(`/etc/profile`) is

```bash
PS1='\h% '
```

#### SC21 BOF

Andrew sent the agenda, "not a big deal."

- [name=Alan:] How are we gathering responses? Some xp using Socrative, setup
  polls to get a feel and launch discussion. It takes time to configure though.
  Alternatives exist: Zoom polls, Slido, Codi

Related, tangentially: Feedback from
[Benson and Annajiat's learners on Slack](https://swcarpentry.slack.com/archives/CEXAZR52T/p1630586431001000?thread_ts=1630519991.000700&cid=CEXAZR52T)
is hard to find & use, can it be transcribed into a readable issue form in the
repo?

#### Other Business

- [name=Alan] has 500 AWS credits that evaporate at the end of November, and
  would like to burn some of it on HPC Carpentry.
  - Amazon is interested in a lesson template for their
    ["ParallelCluster"](https://aws.amazon.com/hpc/parallelcluster/)
    cluster-in-the-cloud, but it's business-oriented, and intended for a single
    user to have a personal cluster. Creating new users requires a lot of
    knowledge of the cloud infrastructure to make it work.
  - Can be handled via Ansible or
    [Clush](https://clustershell.readthedocs.io/en/latest/tools/clush.html)
  - Emerging plan to actually run an HPC Intro workshop in the existing Amazon
    cluster some time in the next month or so.
- Adjacent issues: Can you get past this the user-logistics by providing each
  learner with their own machine?
  - Pro: Sidesteps all the logistical shared-resource set-up. The point is to
    see how the application behaves better in a scaled up/out parallel
    environment, and this does that.
    - People who don't have access to HPC will find it much more accessible &
      useful to spin up a higher-performance cloud instance to get some work
      done. There is **_undeniable utility_** for our community here, though it
      is not a neat fit for the HPC Intro material.
  - Con: Is a poor model for how users are likely to actually scale up/out at a
    permanent facility, which _will_ be a shared resource with a queuing system
    and so forth. The constraints of the shared environment are part of the
    point of what we're trying to convey to learners.

### 2200 UTC session

#### Attending:

- Andrew Reid (he/him), NIST
- Trevor Keller (they/them), NIST
- Wirawan Purwanto (he/him), ODU

#### Review of the morning's discussion

An oblique use-case, for which we do not have material: researchers who do not
have institutional access to HPC machinery, for whom it is valuable to spin up
an AWS instance to perform intensive computing tasks.

#### Workshop Feedback

Questions regarding workshop feedback

- [name=wirawan] Where to post/share workshop experience feedback?
  - we will rehash the learners' feedback but not dump the raw data there
  - for actionable feedback, the consensus is to send it to the respective
    material's issue page (e.g. to `hpc-intro`)
- How to make all these experiential notes discoverable?

#### Workshop Report @ ODU

- [name=wirawan] ODU Research Computing taught "Intro to HPC" workshop for ODU
  HPC users.

Topic taught: `hpc-intro` (only) Length: 2.5 hours Date: 2021-10-07

Notes on materials:

- customized to include a test parallel job using "SU2" package (as a black box
  instead of a fully fleshed job preparation from scratch)
- skipped the parallel run,
  https://carpentries-incubator.github.io/hpc-intro/16-parallel/index.html
- added material on how to _use_ our containerized software (Singularity-based)
- added post-job "resource usage analysis" using sacct to help researchers
  figure out their resource usage after the fact.
- basic file transfer was taught using Open OnDemand File Explorer interface
  (web-based) instead of SCP.

Delivery:

- Taught using "Open OnDemand >> login node shell" interface.

Post-workshop survey:

- Done using Qualtrics, using custom survey questions.
- Initially based on the Carpentries post-surveys then augmented with questions
  such as: their input on the length of the workshop, the level of materials,
  the day/time of the event, ...

Generally the workshop went well. Not sure what else to add until I have chance
to review the survey response.

- [name=Andrew] Add conspicuous pain points & "going very well" points.
- [name=Wirawan] I don't recall a particularly bad pain points.
- We could design additional exercise to help reinforce the learning.
